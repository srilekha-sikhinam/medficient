{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Length of Stay Model Building and Exploration</h3><br />\n",
    "This notebook shows the process of building classification models for length of stay. It shows the performance of models with different bin sizes, different models, and upsampling techniques. This notebook also contains an evaluation of model performance across sub-populations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all needed models\n",
    "import sys\n",
    "sys.path.insert(0, '../src/helpers')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import log_loss, f1_score, classification_report, make_scorer, precision_score, recall_score, accuracy_score, confusion_matrix, plot_confusion_matrix\n",
    "import seaborn as sns\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from model_building_helpers import *\n",
    "from data_cleaners import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all patient csv (csv containing data for all the indications (COPD, Heart Failure, Schizophrenia, Knee Replacement, Kidney/UTI))\n",
    "all_patient_df = load_data('All')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following two cells to see how changing the bin definitions changes the accuracy and confusion matrix of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bin the data by length of stay ranges\n",
    "bins = [1, 5, 15, 30, 45, 60, 90, 120]\n",
    "labels = ['1 to 5', '6 to 15', '16 to 30', '31 to 45', '46 to 60', '60 to 90', '90 to 120']\n",
    "\n",
    "all_patient_df_bins = all_patient_df.copy()\n",
    "all_patient_df_bins['Length of Stay Bin'] = pd.cut(x = all_patient_df_bins['Length of Stay'], bins = bins, labels = labels, include_lowest = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, X_train, X_test, y_train, y_test = get_train_test_data(all_patient_df_bins)\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, np.ravel(y_train))\n",
    "y_pred = model.predict(X_test)\n",
    "bin1_accuracy = accuracy_score(y_test, y_pred)\n",
    "bin1_f1_score_macro = f1_score(y_test, y_pred, average='macro')\n",
    "bin1_f1_score_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Accuracy = \", bin1_accuracy, \"\\nF1 Macro Score = \", bin1_f1_score_macro, \"\\nF1 Weighted Score = \", bin1_f1_score_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the confusion matrix as a heart map\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix), index = [i for i in labels],\n",
    "                  columns = [i for i in labels])\n",
    "plt.figure(figsize = (12, 12))\n",
    "plt.title('Confusion Matrix For Random Forest Model', fontsize=17, weight='bold', pad=30)\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2%',  cmap='Blues')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get RandomForest best parameters and best score by sub-population\n",
    "#Experiment model performance by changing the parameters tested and interations for the RandomizedSearch\n",
    "rf_scores = []\n",
    "iterations = 10 #We found the iterations did not affect F1 macro scores much, so we used the default of 10\n",
    "\n",
    "for drg in ['all', 194.0, 140.0, 750.0, 463.0, 302.0]:\n",
    "    subpop_df = load_data(drg)\n",
    "    subpop_df['Length of Stay Bin'] = pd.cut(x = subpop_df['Length of Stay'], bins = bins, labels = labels, include_lowest = True)\n",
    "    if len(subpop_df) > 30000:\n",
    "        subpop_df = subpop_df.groupby('Length of Stay Bin', group_keys=False).apply(lambda x: x.sample(int(np.rint(30000*len(x)/len(subpop_df))))).sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    X, y, X_train, X_test, y_train, y_test = get_train_test_data(subpop_df)\n",
    "\n",
    "    random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 100, stop = 800, num = 15)],\n",
    "               'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "               'min_samples_split': [2, 5, 10],\n",
    "               'min_samples_leaf': [1, 2, 4],\n",
    "               'bootstrap': [True, False],\n",
    "               'class_weight': ['balanced', 'balanced_subsample']}\n",
    "    \n",
    "    best_params, best_score = get_best_rf_params(X_train, y_train, random_grid, iterations)\n",
    "    rf_scores.append({drg:{\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get LogisticRegression best parameters and best score by sub-population\n",
    "#Experiment model performance by changing the parameters tested and interations for the RandomizedSearch\n",
    "lr_scores = []\n",
    "iterations = 10 #We found the iterations did not affect F1 macro scores much, so we used the default of 10\n",
    "\n",
    "for drg in ['all', 194.0, 140.0, 750.0, 463.0, 302.0]:\n",
    "    subpop_df = load_data(drg)\n",
    "    subpop_df['Length of Stay Bin'] = pd.cut(x = subpop_df['Length of Stay'], bins = bins, labels = labels, include_lowest = True)\n",
    "    if len(subpop_df) > 30000:\n",
    "        subpop_df = subpop_df.groupby('Length of Stay Bin', group_keys=False).apply(lambda x: x.sample(int(np.rint(30000*len(x)/len(subpop_df))))).sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "    X, y, X_train, X_test, y_train, y_test = get_train_test_data(subpop_df)\n",
    "\n",
    "    random_grid = {'C' : np.logspace(0, 4, num=10),\n",
    "    'penalty' : ['l1', 'l2'],\n",
    "    'solver' : ['liblinear'],\n",
    "    'class_weight': ['balanced', None]}\n",
    "    \n",
    "    best_params, best_score = get_best_lr_params(X_train, y_train, random_grid, iterations)\n",
    "    lr_scores.append({drg:{\n",
    "            'best_params': best_params,\n",
    "            'best_score': best_score\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_rf_df = pd.DataFrame(columns=['model', 'subpopulation', 'best_params', 'best_scores'])\n",
    "for item in rf_scores:\n",
    "    for key, item in item.items():\n",
    "        scores_rf_df = scores_rf_df.append({'model': 'RandomForestClassifier', 'subpopulation': key, 'best_params': item['best_params'], 'best_scores': item['best_score']}, ignore_index=True)\n",
    "scores_rf_df.head()\n",
    "\n",
    "scores_lr_df = pd.DataFrame(columns=['model', 'subpopulation', 'best_params', 'best_scores'])\n",
    "for item in lr_scores:\n",
    "    for key, item in item.items():\n",
    "        scores_lr_df = scores_lr_df.append({'model': 'LogisticRegression', 'subpopulation': key, 'best_params': item['best_params'], 'best_scores': item['best_score']}, ignore_index=True)\n",
    "scores_lr_df.head()\n",
    "\n",
    "randomized_search_results = pd.concat([scores_rf_df,scores_lr_df])\n",
    "print(randomized_search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create visualization for each subpopulation for each model\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 5), sharey=True)\n",
    "\n",
    "fig.subplots_adjust(hspace=0.75, wspace=0.125)\n",
    "\n",
    "\n",
    "fig.suptitle('F1 - Macro Scores For LogisticRegression vs RandomForestClassifier')\n",
    "\n",
    "all_patients = randomized_search_results[randomized_search_results['subpopulation'] == 'all']\n",
    "heart_patients = randomized_search_results[randomized_search_results['subpopulation'] == 194.0]\n",
    "copd_patients = randomized_search_results[randomized_search_results['subpopulation'] == 140.0]\n",
    "schizophrenia_patients = randomized_search_results[randomized_search_results['subpopulation'] == 750.0]\n",
    "kidney_patients = randomized_search_results[randomized_search_results['subpopulation'] == 463.0]\n",
    "knee_rep_patients = randomized_search_results[randomized_search_results['subpopulation'] == 302.0]\n",
    "\n",
    "sns.barplot(ax=axes[0, 0], x=all_patients['model'], y=all_patients['best_scores'], palette=['#5CED73', 'skyblue'])\n",
    "axes[0, 0].set_title('All Patients')\n",
    "\n",
    "sns.barplot(ax=axes[0, 1], x=heart_patients['model'], y=heart_patients['best_scores'], palette=['#5CED73', 'skyblue'])\n",
    "axes[0, 1].set_title('Patients with DRG Code 194 (Heart Failure)')\n",
    "\n",
    "sns.barplot(ax=axes[0, 2], x=copd_patients['model'], y=copd_patients['best_scores'], palette=['#5CED73', 'skyblue'])\n",
    "axes[0, 2].set_title('Patients with DRG Code 140 (COPD)')\n",
    "\n",
    "sns.barplot(ax=axes[1, 0], x=schizophrenia_patients['model'], y=schizophrenia_patients['best_scores'], palette=['#5CED73', 'skyblue'])\n",
    "axes[1, 0].set_title('Patients with DRG Code 750 (Schizophrenia)')\n",
    "\n",
    "sns.barplot(ax=axes[1, 1], x=kidney_patients['model'], y=kidney_patients['best_scores'], palette=['#5CED73', 'skyblue'])\n",
    "axes[1, 1].set_title('Patients with DRG Code 463 (Kidney/UTI)')\n",
    "\n",
    "sns.barplot(ax=axes[1, 2], x=knee_rep_patients['model'], y=knee_rep_patients['best_scores'], palette=['#5CED73', 'skyblue'])\n",
    "axes[1, 2].set_title('Patients with DRG Code 302 (Knee Joint Replacement)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like for most models, the RandomForestClassifier does as good or better than the LogisticRegression model, with the exception of the models trained on Knee Replacement patients. Let's train each subpopulation on their best parameters for RandomForestClassifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model_scores = []\n",
    "\n",
    "for drg in ['all', 194.0, 140.0, 750.0, 463.0, 302.0]:\n",
    "    subpop_df = load_data(drg)\n",
    "    subpop_df['Length of Stay Bin'] = pd.cut(x = subpop_df['Length of Stay'], bins = bins, labels = labels, include_lowest = True)\n",
    "      \n",
    "    X, y, X_train, X_test, y_train, y_test = get_train_test_data(subpop_df)\n",
    "\n",
    "    best_params = scores_rf_df.loc[(scores_rf_df['model']=='RandomForestClassifier') & (scores_rf_df['subpopulation']==drg)]['best_params'].values[0]\n",
    "    model_scores = get_model_scores(best_params, X_train, X_test, y_train, y_test)\n",
    "    final_model_scores.append({\n",
    "        'Population': drg,\n",
    "        'Accuracy Score': model_scores[0],\n",
    "        'F1_score_macro': model_scores[1],\n",
    "        'F1_score_weighted': model_scores[2]\n",
    "    })\n",
    "\n",
    "final_results_df = pd.DataFrame(final_model_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the results\n",
    "#Build visualization comparing random forest model performance of all populations\n",
    "colors = ['orangered' if (x == 'all') else 'skyblue' for x in final_results_df['Population'].values]\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 16), sharey=True)\n",
    "fig.subplots_adjust(hspace=0.45, wspace=0.25)\n",
    "fig.suptitle('Comparison of Random Forest Model Performace By Subpopulation', fontsize=16)\n",
    "\n",
    "sns.barplot(ax=axes[0], x=final_results_df['Population'], y=final_results_df['F1_score_macro'], palette=colors)\n",
    "axes[0].set_xlabel('Subpopulation', fontsize=12)\n",
    "axes[0].set_ylabel('F1 Macro Score', fontsize=12)\n",
    "axes[0].bar_label(axes[0].containers[0])\n",
    "axes[0].set_title('F1 Macro Score Across Populations')\n",
    "\n",
    "sns.barplot(ax=axes[1], x=final_results_df['Population'], y=final_results_df['F1_score_weighted'], palette=colors)\n",
    "axes[1].set_xlabel('Subpopulation', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Weighted Score', fontsize=12)\n",
    "axes[1].bar_label(axes[1].containers[0])\n",
    "axes[1].set_title('F1 Weighted Score Across Populations')\n",
    "\n",
    "sns.barplot(ax=axes[2], x=final_results_df['Population'], y=final_results_df['Accuracy Score'], palette=colors)\n",
    "axes[2].set_xlabel('Subpopulation', fontsize=12)\n",
    "axes[2].set_ylabel('Accuracy Score', fontsize=12)\n",
    "axes[2].bar_label(axes[2].containers[0])\n",
    "axes[2].set_title('Accuracy Score Across Populations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the visualization above, it can be seen that for F1 macro scores the model built on all conditions of interest outperforms models trained only on patients with a specific DRG code with the exception of Knee replacement patients.\n",
    "<br /><br />\n",
    "Let's also compare performance with a dummy classifier which predicts the majority class every time and a dummy classifier which selects a class at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get dummy model performance on all the population datasets\n",
    "final_model_scores = []\n",
    "\n",
    "#all population\n",
    "for drg in ['All', 194.0, 140.0, 750.0, 463.0, 302.0]:\n",
    "    print(drg)\n",
    "    subpop_df = load_data(drg)\n",
    "    subpop_df['Length of Stay Bin'] = pd.cut(x = subpop_df['Length of Stay'], bins = bins, labels = labels, include_lowest = True)\n",
    "      \n",
    "    X, y, X_train, X_test, y_train, y_test = get_train_test_data(subpop_df)\n",
    "    \n",
    "    most_freq_dummy_scores = get_dummy_scores(\"most_frequent\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    final_model_scores.append({\n",
    "        'Population': drg,\n",
    "        'Model': 'DummyClassifier - Most Frequent',\n",
    "        'Accuracy Score': most_freq_dummy_scores[0],\n",
    "        'F1_score_macro': most_freq_dummy_scores[1],\n",
    "        'F1_score_weighted': most_freq_dummy_scores[2]\n",
    "    })\n",
    "\n",
    "    random_dummy_scores = get_dummy_scores(\"uniform\", X_train, X_test, y_train, y_test)\n",
    "\n",
    "    final_model_scores.append({\n",
    "        'Population': drg,\n",
    "        'Model': 'DummyClassifier - Random',\n",
    "        'Accuracy Score': random_dummy_scores[0],\n",
    "        'F1_score_macro': random_dummy_scores[1],\n",
    "        'F1_score_weighted': random_dummy_scores[2]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores_df = pd.DataFrame(final_model_scores)\n",
    "final_results_with_dummy_df = pd.concat([final_results_df, final_scores_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot dummy classifiers vs rf model performance for each population\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 18), sharey=True)\n",
    "fig.subplots_adjust(hspace=0.45, wspace=0.25)\n",
    "fig.suptitle('Scores of Trained Random Forest Model vs. Dummy Models By Subpopulation', fontsize=16)\n",
    "\n",
    "sns.barplot(ax=axes[0], data=final_results_with_dummy_df, x=\"Population\", y=\"F1_score_macro\", hue=\"Model\",palette= ['orangered', 'skyblue', '#5CED73'])\n",
    "axes[0].set_xlabel('Subpopulation', fontsize=12)\n",
    "axes[0].set_ylabel('F1 Macro Score', fontsize=12)\n",
    "#axes[0].bar_label(axes[0].containers[0])\n",
    "axes[0].set_title('F1 Macro Scores')\n",
    "\n",
    "sns.barplot(ax=axes[1], data=final_results_with_dummy_df, x=\"Population\", y=\"F1_score_weighted\", hue=\"Model\",palette= ['orangered', 'skyblue', '#5CED73'])\n",
    "axes[1].set_xlabel('Subpopulation', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Weighted Score', fontsize=12)\n",
    "#axes[1].bar_label(axes[1].containers[0])\n",
    "axes[1].set_title('F1 Weighted Scores')\n",
    "\n",
    "sns.barplot(ax=axes[2], data=final_results_with_dummy_df, x=\"Population\", y=\"Accuracy Score\", hue=\"Model\",palette= ['orangered', 'skyblue', '#5CED73'])\n",
    "axes[2].set_xlabel('Subpopulation', fontsize=12)\n",
    "axes[2].set_ylabel('Accuracy Score', fontsize=12)\n",
    "#axes[2].bar_label(axes[2].containers[0])\n",
    "axes[2].set_title('Accuracy Scores')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models do outperform the dummy models, especially when comparing the F1 Macro score. However, overall we can see that the F1 macro scores are still fairly low for the trained models. This is due to the class imbalance we see in the dataset. About 65% of patients stay in the hospital 1 - 5 days. This is about the same as our accuracy and weighted F1 scores.\n",
    "<br /><br />\n",
    "As a next step, let's see if upsampling our tarining data for the 'All patients' model makes a difference to the F1 macro score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample_scores = []\n",
    "\n",
    "oversample = SMOTE()\n",
    "\n",
    "best_params = scores_rf_df.loc[(scores_rf_df['model']=='RandomForestClassifier') & (scores_rf_df['subpopulation']=='all')]['best_params'].values[0]\n",
    "\n",
    "subpop_df = load_data('all')\n",
    "subpop_df['Length of Stay Bin'] = pd.cut(x = subpop_df['Length of Stay'], bins = bins, labels = labels, include_lowest = True)\n",
    "      \n",
    "X, y, X_train, X_test, y_train, y_test = get_train_test_data(subpop_df)\n",
    "\n",
    "X_train_resample, y_train_resample = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "model_scores = get_model_scores(best_params, X_train, X_test, y_train, y_test)\n",
    "\n",
    "oversample_scores.append({\n",
    "    'Population': 'all',\n",
    "    'Model': 'RandomForestClassifier - Upsampled',\n",
    "    'Accuracy Score': model_scores[0],\n",
    "    'F1_score_macro': model_scores[1],\n",
    "    'F1_score_weighted': model_scores[2]\n",
    "})\n",
    "\n",
    "final_results_df = final_results_df.append(oversample_scores)\n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot dummy classifiers vs rf model performance for each population\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 18), sharey=True)\n",
    "fig.subplots_adjust(hspace=0.45, wspace=0.25)\n",
    "fig.suptitle('Scores of Trained Random Forest Model vs. Dummy Models By Subpopulation', fontsize=16)\n",
    "\n",
    "final_results_upsampled = final_results_df[final_results_df['Population'] == 'all']\n",
    "sns.barplot(ax=axes[0], data=final_results_df, x=\"Model\", y=\"F1_score_macro\", palette= ['orangered', 'skyblue'])\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "axes[0].set_ylabel('F1 Macro Score', fontsize=12)\n",
    "#axes[0].bar_label(axes[0].containers[0])\n",
    "axes[0].set_title('F1 Macro Scores')\n",
    "\n",
    "sns.barplot(ax=axes[1], data=final_results_df, x=\"Model\", y=\"F1_score_weighted\", palette= ['orangered', 'skyblue'])\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "axes[1].set_ylabel('F1 Weighted Score', fontsize=12)\n",
    "#axes[1].bar_label(axes[1].containers[0])\n",
    "axes[1].set_title('F1 Weighted Scores')\n",
    "\n",
    "sns.barplot(ax=axes[2], data=final_results_df, x=\"Model\", y=\"Accuracy Score\", palette= ['orangered', 'skyblue'])\n",
    "axes[2].set_xlabel('Model', fontsize=12)\n",
    "axes[2].set_ylabel('Accuracy Score', fontsize=12)\n",
    "#axes[2].bar_label(axes[2].containers[0])\n",
    "axes[2].set_title('Accuracy Scores')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eabca979b0553fa6d87e9a00c352604d3b703d4afc9641643dd42376492b80f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
